import asyncio
import logging
import time

from typing import Dict

from pandas import DataFrame
from langchain_core.messages import HumanMessage, SystemMessage

from src.chat.ai.managers.huggingface_manager import get_hf_manager
from src.chat.ai.managers.ollama_manager import get_ollama_manager
from src.chat.core.configs import settings
from src.chat.core.logging_config import setup_logging
from src.chat.model.agent import Agent
from src.chat.model.chat_models import OllamaModel, HuggingFaceModel
from src.chat.tools.tokenizer import get_token_counter

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤
TEST_PROMPT = "–û–±—ä—è—Å–Ω–∏ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –≥–ª—É–±–æ–∫–∏–º –æ–±—É—á–µ–Ω–∏–µ–º –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏."
SYSTEM_PROMPT = "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä—è—Å–Ω—è–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º."
MAX_TOKENS = 512
TEMPERATURE = 0

MODELS_TO_TEST: list[Agent] = [
    # Ollama models
    Agent(
        agent_id="test_ollama_tiny",
        provider="ollama",
        name="tinyllama:latest",
        temperature=TEMPERATURE,
        model=OllamaModel.TINYLLAMA.value,
        max_tokens=MAX_TOKENS
    ),
    Agent(
        agent_id="test_ollama_mistral",
        provider="ollama",
        name="mistral:7b",
        temperature=TEMPERATURE,
        model=OllamaModel.MISTRAL_7B.value,
        max_tokens=MAX_TOKENS
    ),
    Agent(
        agent_id="test_ollama_llama2",
        provider="ollama",
        name="llama2:13b",
        temperature=TEMPERATURE,
        model=OllamaModel.LLAMA2_13B.value,
        max_tokens=MAX_TOKENS
    ),

    # HuggingFace models
    Agent(
        agent_id="test_hf_mistral",
        provider="huggingface",
        name=HuggingFaceModel.MISTRAL_7B_INSTRUCT.value,
        temperature=TEMPERATURE,
        model=HuggingFaceModel.MISTRAL_7B_INSTRUCT.value,
        max_tokens=MAX_TOKENS
    ),

    Agent(
        agent_id="test_hf_llama",
        provider="huggingface",
        name=HuggingFaceModel.LLAMA_3_1_8B_INSTRUCT.value,
        temperature=TEMPERATURE,
        model=HuggingFaceModel.LLAMA_3_1_8B_INSTRUCT.value,
        max_tokens=MAX_TOKENS
    ),

    Agent(
        agent_id="test_hf_sao10k",
        provider="huggingface",
        name=HuggingFaceModel.SAO10K_L3_8B_STHENO_V3_2.value,
        temperature=TEMPERATURE,
        model=HuggingFaceModel.SAO10K_L3_8B_STHENO_V3_2.value,
        max_tokens=MAX_TOKENS
    ),
]


async def run_model(agent: Agent) -> Dict:
    """–¢–µ—Å—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∑–∞–º–µ—Ä–æ–º –º–µ—Ç—Ä–∏–∫"""
    print(f"\n{'='*60}")
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {agent.name}")
    print(f"{'='*60}")


    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    messages = [
        SystemMessage(SYSTEM_PROMPT),
        HumanMessage(TEST_PROMPT)
    ]

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    token_counter = get_token_counter()
    logging.info(f"token_counter for {agent.provider}")
    input_tokens = token_counter.count_message_tokens(messages, agent.name)

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏
    start_time = time.time()
    logging.info(f"start_time for {agent.provider}")
    try:
        if agent.provider == "ollama":
            manager = get_ollama_manager()
            response = await manager.ainvoke(agent, messages)
        elif agent.provider == "huggingface":
            manager = get_hf_manager()  # type: ignore
            response = await manager.ainvoke(agent, messages)
        else:
            raise ValueError(f"Unknown provider: {agent.provider}")

        response_time = time.time() - start_time

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        response_text: str = str(response.content) if hasattr(response, 'content') else str(response)
        output_tokens = token_counter.count_tokens(response_text, agent.model)
        total_tokens = input_tokens + output_tokens

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ! –í—Ä–µ–º—è: {response_time:.2f}s")
        print(f"üìä –¢–æ–∫–µ–Ω—ã: Input={input_tokens}, Output={output_tokens}, Total={total_tokens}")
        print(f"üí¨ –û—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):\n{response_text[:200]}...")

        return {
            "model": agent.name,
            "provider": agent.provider,
            "response_time_s": round(response_time, 2),
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "response": response_text,
            "status": "success"
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return {
            "model": agent.name,
            "provider": agent.provider,
            "response_time_s": -1,
            "input_tokens": input_tokens,
            "output_tokens": 0,
            "total_tokens": input_tokens,
            "response": f"ERROR: {str(e)}",
            "status": "error"
        }


async def main()->None:
    setup_logging()
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    print(f"üìù –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç: {TEST_PROMPT}\n")

    results = []

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–º–æ–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å, –Ω–æ API rate limits)
    for model_config in MODELS_TO_TEST:
        result = await run_model(model_config)
        results.append(result)

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—É–≤–∞–∂–∞–µ–º API rate limits)
        await asyncio.sleep(2)

    # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    df = DataFrame(results)

    print("\n" + "="*80)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print("="*80)
    print(df[['model', 'provider', 'response_time_s', 'input_tokens',
              'output_tokens', 'total_tokens', 'status']].to_string(index=False))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    csv_filename = f"model_comparison_results_{int(time.time())}.csv"
    df.to_csv(str(settings.DATA_DIR / csv_filename), index=False, encoding='utf-8')
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {csv_filename}")

    # –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ - —Ç–æ–ø –º–æ–¥–µ–ª–∏
    print("\n" + "="*80)
    print("üèÜ –ú–û–î–ï–õ–ò:")
    print("="*80)

    successful = df[df['status'] == 'success']
    if not successful.empty:
        print(f"‚ö° –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {successful.loc[successful['response_time_s'].idxmin(), 'model']} "
              f"({successful['response_time_s'].min():.2f}s)")
        print(f"üí∞ –ù–∞–∏–º–µ–Ω—å—à–∏–π —Ä–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤: {successful.loc[successful['total_tokens'].idxmin(), 'model']} "
              f"({successful['total_tokens'].min()} tokens)")


if __name__ == "__main__":
    asyncio.run(main())
